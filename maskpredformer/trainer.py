# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_trainer.ipynb.

# %% auto 0
__all__ = ['MaskSimVPTrainer']

# %% ../nbs/02_trainer.ipynb 2
import torch
import os

import lightning as pl
from lightning.pytorch.loggers import WandbLogger
from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor

import matplotlib.pyplot as plt
from torchvision import transforms
import wandb
import random
import numpy as np
from lightning.pytorch.utilities import grad_norm

from .mask_simvp import MaskSimVP
from .simvp_dataset import DLDataset

# %% ../nbs/02_trainer.ipynb 3
class MaskSimVPTrainer(pl.LightningModule):
    def __init__(self, 
                 in_shape, hid_S, hid_T, N_S, N_T, model_type,
                 batch_size, lr, weight_decay, max_epochs,
                 data_root):
        super().__init__()
        self.save_hyperparameters()
        self.model = MaskSimVP(
            in_shape, hid_S, hid_T, N_S, N_T, model_type
        )
        self.train_set = DLDataset(data_root, "train")
        self.val_set = DLDataset(data_root, "val")
        self.criterion = torch.nn.CrossEntropyLoss()
    
    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            self.train_set, batch_size=self.hparams.batch_size, 
            num_workers=8, shuffle=True, pin_memory=True
        )

    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            self.val_set, batch_size=self.hparams.batch_size, 
            num_workers=8, shuffle=False, pin_memory=True
        )

    def step(self, x, y):
        
        pre_seq_len = x.shape[1]
        after_seq_len = y.shape[1]

        d = after_seq_len // pre_seq_len
        m = after_seq_len % pre_seq_len

        pred_y = [] 
        cur_seq = x.clone()
        for _ in range(d):
            cur_seq_logits = self.model(cur_seq)
            cur_seq = cur_seq_logits.argmax(dim=2) # (B, T, 49, H, W) -> (B, T, H, W)
            pred_y.append(cur_seq_logits)
        
        if m != 0:
            cur_seq_logits = self.model(cur_seq)
            pred_y.append(cur_seq_logits[:, :m])

        pred_y = torch.cat(pred_y, dim=1)

        assert pred_y.shape == (y.shape[0], y.shape[1], 49, y.shape[3], y.shape[4]) 
        return pred_y
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        pred_y = self.step(x, y)
        loss = self.criterion(pred_y, y)
        self.log("train_loss", loss)
        return loss
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        pred_y = self.step(x, y)
        loss = self.criterion(pred_y, y)
        self.log("val_loss", loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.parameters(), lr=self.hparams.lr, 
            weight_decay=self.hparams.weight_decay
        )
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer, max_lr=self.hparams.lr,
            total_steps=self.hparams.max_epochs*len(self.train_dataloader()),
            final_div_factor=1e4
        )
        return [optimizer], [scheduler]
