{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import os \n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_masks = torch.load('../data/DL/val_masks.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WenmaSet(Dataset):\n",
    "    def __init__(self, data_path, data_type, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.data_type = data_type\n",
    "        self.transform = transform\n",
    "\n",
    "        if (\n",
    "            self.data_type == \"train\"\n",
    "            or self.data_type == \"val\"\n",
    "            or self.data_type == \"unlabeled\"\n",
    "        ):\n",
    "            self.num_frames = 22\n",
    "\n",
    "        else:\n",
    "            self.num_frames = 11\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        images = []\n",
    "        masks = []\n",
    "\n",
    "        if \"train\" in self.data_type:\n",
    "            ind = ind\n",
    "\n",
    "        elif \"val\" in self.data_type:\n",
    "            ind = ind + 1000\n",
    "\n",
    "        elif \"unlabeled\" in self.data_type:\n",
    "            ind = ind + 2000\n",
    "\n",
    "        elif \"hidden\" in self.data_type:\n",
    "            ind = ind + 15000\n",
    "\n",
    "        video_path = os.path.join(self.data_path, \"video_{}\".format(ind))\n",
    "\n",
    "        if \"hidden\" in self.data_type or \"unlabeled\" in self.data_type:\n",
    "            mask_path = None\n",
    "\n",
    "        else:\n",
    "            mask_path = os.path.join(video_path, \"mask.npy\")\n",
    "\n",
    "        for frame in range(self.num_frames):\n",
    "            image_path = os.path.join(video_path, \"image_{}.png\".format(frame))\n",
    "            image = np.array(Image.open(image_path))\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            images.append(image)\n",
    "            if mask_path != None:\n",
    "                if \"prediction\" in self.data_type:\n",
    "                    mask = np.load(mask_path)[frame + 11]\n",
    "                else:\n",
    "                    mask = np.load(mask_path)[frame]\n",
    "            else:\n",
    "                mask = torch.zeros((160, 240))\n",
    "            masks.append(mask)\n",
    "\n",
    "        return images, masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('checkpoints/unet9.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Resize((160, 240)),          \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "           \n",
    "])\n",
    "data_path = \"data/Dataset_Student/\"\n",
    "data_type = \"val\"\n",
    "\n",
    "dataset= WenmaSet(data_path = data_path + data_type,\n",
    "                  data_type = data_type,\n",
    "                  transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_predictions = []\n",
    "for i in range(22):\n",
    "    image = imgs[i].unsqueeze(0).to(device)\n",
    "    mask_prediction = model(image)\n",
    "    mask_predictions.append(mask_prediction)\n",
    "mask_predictions = [torch.argmax(pred, dim=1).detach().cpu() for pred in mask_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_predictions[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(mask_predictions[10][0], pred_val_masks[0][10].long())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
