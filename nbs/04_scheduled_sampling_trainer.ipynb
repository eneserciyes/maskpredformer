{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Trainer for MaskSimVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scheduled_sampling_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import lightning as pl\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import math\n",
    "import random\n",
    "\n",
    "from maskpredformer.mask_simvp import MaskSimVP\n",
    "from maskpredformer.vis_utils import show_gif, show_video_line\n",
    "from maskpredformer.simvp_dataset import DLDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def inv_sigmoid_schedule(x, n, k):\n",
    "    y = k / (k+math.exp(((x-(n//2))/(n//20))/k))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskSimVPScheduledSamplingModule(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 in_shape, hid_S, hid_T, N_S, N_T, model_type,\n",
    "                 batch_size, lr, weight_decay, max_epochs, data_root,\n",
    "                 sample_step_inc_every_n_epoch, schedule_k=2, max_sample_steps=5,\n",
    "                 pre_seq_len=11, aft_seq_len=1, drop_path=0.0, unlabeled=False, downsample=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = MaskSimVP(\n",
    "            in_shape, hid_S, hid_T, N_S, N_T, model_type, downsample=downsample, drop_path=drop_path,\n",
    "            pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len\n",
    "        )\n",
    "        self.train_set = DLDataset(data_root, \"train\", unlabeled=unlabeled, pre_seq_len=pre_seq_len, aft_seq_len=max_sample_steps+1)\n",
    "        self.val_set = DLDataset(data_root, \"val\", pre_seq_len=11, aft_seq_len=11)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.iou_metric = JaccardIndex(task='multiclass', num_classes=49)\n",
    "        \n",
    "        self.schedule_idx = 0\n",
    "        self.sample_steps = 1\n",
    "        self.schedule_max = len(self.train_dataloader()) * sample_step_inc_every_n_epoch\n",
    "\n",
    "    def sample_or_not(self):\n",
    "        assert self.schedule_idx < self.schedule_max, \"Schedule idx larger than max, something wrong with schedule\"\n",
    "\n",
    "        p = 1 - inv_sigmoid_schedule(self.schedule_idx, self.schedule_max, self.hparams.schedule_k)\n",
    "        self.schedule_idx += 1\n",
    "        return random.random() < p\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_autoregressive(self, x, t):\n",
    "        cur_seq = x.clone()\n",
    "        for _ in range(t):\n",
    "            y_hat_logit_t = self.model(cur_seq)\n",
    "            y_hat = torch.argmax(y_hat_logit_t, dim=2) # get current prediction\n",
    "            cur_seq = torch.cat([cur_seq[:, 1:], y_hat], dim=1) # autoregressive concatenation\n",
    "        return cur_seq\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if self.sample_or_not():\n",
    "            x = self.sample_autoregressive(x, self.sample_steps)\n",
    "            y = y[:, self.sample_steps:self.sample_steps+1] # get the next label after sampling model `sample_steps` times\n",
    "        else:\n",
    "            # no change in x\n",
    "            y = y[:, 0:1] # get the normal training label\n",
    "        \n",
    "        y_hat_logits = self.model(x)\n",
    "        \n",
    "        # Flatten batch and time dimensions\n",
    "        b, t, *_ = y_hat_logits.shape\n",
    "        y_hat_logits = y_hat_logits.view(b*t, *y_hat_logits.shape[2:])\n",
    "        y = y.view(b*t, *y.shape[2:])\n",
    "\n",
    "        loss = self.criterion(y_hat_logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.sample_autoregressive(x, 11)\n",
    "        iou = self.iou_metric(y_hat[:, -1], y[:, -1])\n",
    "        self.log(\"valid_last_frame_iou\", self.iou_metric, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return iou\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if (self.current_epoch+1) % self.hparams.sample_step_inc_every_n_epoch:\n",
    "            self.schedule_idx = 0\n",
    "            self.sample_steps = max(self.sample_steps+1, self.hparams.max_sample_steps)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.hparams.lr,\n",
    "            total_steps=self.hparams.max_epochs*len(self.train_dataloader()),\n",
    "            final_div_factor=1e4\n",
    "        )\n",
    "        opt_dict = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\":{\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            } \n",
    "        }\n",
    "\n",
    "        return opt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test out the MaskSimVP Scheduled Sampling Lightning Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/simvp_epoch=16-val_loss=0.014.ckpt\"\n",
    "mask_sim_vp_ckpt = torch.load(ckpt_path)\n",
    "\n",
    "ss_params = mask_sim_vp_ckpt['hyper_parameters']\n",
    "ss_params['unlabeled'] = False\n",
    "ss_params['sample_step_inc_every_n_epoch'] = 2\n",
    "ss_params['max_epochs'] = 10\n",
    "ss_params['max_sample_steps'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_module = MaskSimVPScheduledSamplingModule(**ss_params)\n",
    "pl_module.load_state_dict(mask_sim_vp_ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pl_module(pl_module):\n",
    "    x, y = pl_module.train_set[0]\n",
    "    pl_module.schedule_idx = pl_module.schedule_max * 0.95\n",
    "    pl_module.sample_steps = 1\n",
    "    x = x.unsqueeze(0).to(pl_module.device)\n",
    "    y = y.unsqueeze(0).to(pl_module.device)\n",
    "    loss = pl_module.training_step((x,y), 0)\n",
    "    print(loss)\n",
    "test_pl_module(pl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Video Callback\n",
    "\n",
    "> sample video callback to generate video samples during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
