# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_simvp_dataset.ipynb.

# %% auto 0
__all__ = ['DEFAULT_DATA_PATH', 'DLDataset']

# %% ../nbs/01_simvp_dataset.ipynb 2
from torch.utils.data import Dataset
import torch
import os
from torchvision import transforms
from .vis_utils import show_video_line

# %% ../nbs/01_simvp_dataset.ipynb 3
DEFAULT_DATA_PATH = "/home/enes/dev/maskpredformer/data/DL"

# %% ../nbs/01_simvp_dataset.ipynb 4
class DLDataset(Dataset):
    def __init__(self, root, mode, unlabeled=False, pre_seq_len=11, aft_seq_len=11, ep_len=22):
        self.mask_path = os.path.join(root, f"{mode}_masks.pt")
        self.mode = mode
        print("INFO: Loading masks from", self.mask_path)
        if unlabeled:
            self.masks = torch.cat([
                torch.load(self.mask_path), 
                torch.load(os.path.join(root, f"unlabeled_masks.pt")).squeeze()
            ], dim=0)
        else:
            self.masks = torch.load(self.mask_path)
        self.transform = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
        ])
        self.pre_seq_len=pre_seq_len
        self.aft_seq_len=aft_seq_len
        self.seq_per_ep = ep_len - (pre_seq_len + aft_seq_len) + 1

    def __len__(self):
        return self.masks.shape[0] * self.seq_per_ep
    
    def __getitem__(self, idx):
        ep_idx = idx // self.seq_per_ep
        offset = idx % self.seq_per_ep
        total_len = self.pre_seq_len + self.aft_seq_len
        
        if self.mode == "train":
            ep = self.transform(self.masks[ep_idx, offset:offset+total_len])
        else:
            ep = self.masks[ep_idx, offset:offset+total_len]
        data = ep[:self.pre_seq_len].long()
        labels = ep[self.pre_seq_len:].long()
        return data, labels
