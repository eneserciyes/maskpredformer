{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimVP Mask Predictor Model\n",
    "\n",
    "> simvp_mask_predictor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp mask_simvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from openstl.models.simvp_model import SimVP_Model\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DEFAULT_MODEL_CONFIG = {\n",
    "    # For MetaVP models, the most important hyperparameters are: \n",
    "    # N_S, N_T, hid_S, hid_T, model_type\n",
    "    'in_shape': [11, 3, 160, 240],\n",
    "    'hid_S': 64,\n",
    "    'hid_T': 512,\n",
    "    'N_S': 4,\n",
    "    'N_T': 8,\n",
    "    'model_type': 'gSTA',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskSimVP(nn.Module):\n",
    "    def __init__(self, in_shape, hid_S, hid_T, N_S, N_T, model_type):\n",
    "        super().__init__()\n",
    "        c = in_shape[1]\n",
    "        self.simvp = SimVP_Model(\n",
    "            in_shape=in_shape, hid_S=hid_S, \n",
    "            hid_T=hid_T, N_S=N_S, N_T=N_T, \n",
    "            model_type=model_type)\n",
    "        self.token_embeddings = nn.Embedding(49, c)\n",
    "        self.out_conv = nn.Conv2d(c, 49, 1, 1)\n",
    "        self.pre_seq_len = 11\n",
    "        self.after_seq_len = 11\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.token_embeddings(tokens)\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "\n",
    "        d = self.after_seq_len // self.pre_seq_len\n",
    "        m = self.after_seq_len % self.pre_seq_len\n",
    "\n",
    "        y_hat = []\n",
    "        cur_seq = x.clone()\n",
    "        for _ in range(d):\n",
    "            cur_seq = self.simvp(cur_seq)\n",
    "            y_hat.append(cur_seq)\n",
    "        \n",
    "        if m != 0:\n",
    "            cur_seq = self.simvp(cur_seq)\n",
    "            y_hat.append(cur_seq[:, :m])\n",
    "        \n",
    "        y_hat = torch.cat(y_hat, dim=1)\n",
    "\n",
    "        b, t, c, h, w = y_hat.shape\n",
    "        y_hat = y_hat.view(b*t, c, h, w)\n",
    "\n",
    "        y_hat_logits = self.out_conv(y_hat)\n",
    "        y_hat_logits = y_hat_logits.view(b, t, 49, h, w)\n",
    "        return y_hat_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskSimVP(**DEFAULT_MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randint(0, 49, (1, 11, 160, 240)).long()\n",
    "out = model(x)\n",
    "assert out.shape == (1, 11, 49, 160, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
