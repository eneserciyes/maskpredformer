{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd11f12-b7a3-492a-ba8a-3e3ccb05c0c9",
   "metadata": {},
   "source": [
    "# Train predictor using autoregressive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196642d1-bd27-4260-918d-e0a5176e8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp autoregressive_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea93c7d-3eed-4c3a-91ec-fd15e28f986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import lightning as pl\n",
    "import torch\n",
    "from maskpredformer.mask_simvp import MaskSimVP\n",
    "from maskpredformer.simvp_dataset import DLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa323b3-0790-4a7f-80e8-d0d3fac02232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskSimVPAutoRegressiveModule(pl.LightningModule):\n",
    "    def __init__(self, in_shape, hid_S, hid_T, N_S, N_T, model_type,\n",
    "                 batch_size, lr, weight_decay, max_epochs,\n",
    "                 data_root, pre_seq_len=11, aft_seq_len=1,\n",
    "                 drop_path=0.0, unlabeled=False, downsample=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = MaskSimVP(\n",
    "            in_shape, hid_S, hid_T, N_S, N_T, model_type, downsample=downsample, drop_path=drop_path,\n",
    "            pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len\n",
    "        )\n",
    "        self.train_set = DLDataset(data_root, \"train\", unlabeled=unlabeled, pre_seq_len=11, aft_seq_len=11)\n",
    "        self.val_set = DLDataset(data_root, \"val\", pre_seq_len=11, aft_seq_len=11)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def calculate_loss(self, logits, target):\n",
    "        b, t, *_ = logits.shape\n",
    "        logits = logits.view(b*t, *logits.shape[2:])\n",
    "        target = target.view(b*t, *target.shape[2:])\n",
    "        loss = self.criterion(logits, target)\n",
    "        return loss\n",
    "    \n",
    "    def step(self, x, y):\n",
    "        y_hat_logits = []\n",
    "        cur_seq = x.clone()\n",
    "        for _ in range(11):\n",
    "            y_hat_logit_t = self.model(cur_seq)\n",
    "            y_hat_logits.append(y_hat_logit_t) # get logits for backprop\n",
    "            y_hat = torch.argmax(y_hat_logit_t, dim=2) # get current prediction\n",
    "            cur_seq = torch.cat([cur_seq[:, 1:], y_hat], dim=1) # autoregressive concatenation\n",
    "            \n",
    "        y_hat_logits = torch.stack(y_hat_logits)\n",
    "        import pdb; pdb.set_trace()\n",
    "        # calculate loss\n",
    "        loss = self.calculate_loss(y_hat_logits, y)\n",
    "        return loss, y_hat_logits, cur_seq\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, _, _ = self.step(x, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, _, _ = self.step(x, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.hparams.lr,\n",
    "            total_steps=self.hparams.max_epochs*len(self.train_dataloader()),\n",
    "            final_div_factor=1e4\n",
    "        )\n",
    "        opt_dict = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\":{\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            } \n",
    "        }\n",
    "\n",
    "        return opt_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8aa6c-781e-4dd0-b351-68d8071ac331",
   "metadata": {},
   "source": [
    "**Test out the MaskSimVPAutoRegressive Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fee37-ae76-4bf0-b2c7-69fa1bdde957",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d0bb8-6f94-4566-9354-244f3c0b102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sim_vp_ckpt = torch.load(\"checkpoints/simvp_epoch=13-val_loss=0.015.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf754fc-6d79-4d70-82ee-21f8e299a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoregressive_params = mask_sim_vp_ckpt['hyper_parameters']\n",
    "autoregressive_params['unlabeled'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4980b71-b6cb-4dc7-85ef-e5033008f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_module = MaskSimVPAutoRegressiveModule(**autoregressive_params)\n",
    "pl_module.load_state_dict(mask_sim_vp_ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7509d7-f5f5-47bc-9ebb-465f1b879640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prior_model_results():\n",
    "    x, y = pl_module.val_set[0]\n",
    "    x=x.unsqueeze(0).to(pl_module.device); y=y.unsqueeze(0).to(pl_module.device)\n",
    "    loss, y_hat_logits, cur_seq = pl_module.step(x, y)\n",
    "    print(cur_seq.shape)\n",
    "test_prior_model_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
